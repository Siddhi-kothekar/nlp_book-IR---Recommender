{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip -q install pandas numpy scikit-learn sentence-transformers nltk tqdm gradio pyyaml\n",
        "\n",
        "# Avoid TensorFlow imports from transformers\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and helpers\n",
        "from typing import Iterable, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download VADER\n",
        "try:\n",
        "    _ = nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "\n",
        "def l2_normalize(matrix: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    norms = np.linalg.norm(matrix, axis=1, keepdims=True)\n",
        "    norms = np.maximum(norms, eps)\n",
        "    return matrix / norms\n",
        "\n",
        "class VaderSentiment:\n",
        "    def __init__(self):\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "    def score_texts(self, texts: Iterable[str]) -> List[float]:\n",
        "        out = []\n",
        "        for t in texts:\n",
        "            pol = self.analyzer.polarity_scores(t or \"\")\n",
        "            out.append(float(pol.get(\"compound\", 0.0)))\n",
        "        return out\n",
        "\n",
        "class EmbeddingModel:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "    def encode(self, texts: Iterable[str], batch_size: int = 256) -> np.ndarray:\n",
        "        embs = self.model.encode(list(texts), batch_size=batch_size, convert_to_numpy=True, normalize_embeddings=False, show_progress_bar=True)\n",
        "        return l2_normalize(embs)\n",
        "\n",
        "\n",
        "def aggregate_product_scores(\n",
        "    df: pd.DataFrame,\n",
        "    retrieved_indices: np.ndarray,\n",
        "    retrieved_scores: np.ndarray,\n",
        "    review_sentiments: np.ndarray,\n",
        "    k_recommendations: int = 10,\n",
        "    sentiment_weight: float = 0.6,\n",
        "    rating_weight: float = 0.4,\n",
        "    sentiment_min: float = 0.0,\n",
        "    rating_min: float = 4.0,\n",
        ") -> List[Tuple[str, float]]:\n",
        "    asins = df[\"asin\"].to_numpy()\n",
        "    ratings = df[\"overall\"].to_numpy(dtype=float)\n",
        "    product_to_score = {}\n",
        "    product_to_hits = {}\n",
        "    for row_scores, row_idx in zip(retrieved_scores, retrieved_indices):\n",
        "        for s, i in zip(row_scores, row_idx):\n",
        "            sim = float(s)\n",
        "            if sim <= 0.0:\n",
        "                continue\n",
        "            asin = asins[i]\n",
        "            rating = ratings[i]\n",
        "            sent = float(review_sentiments[i])\n",
        "            if sent < sentiment_min or rating < rating_min:\n",
        "                continue\n",
        "            review_score = sim * (1.0 + sentiment_weight * sent) * (1.0 + rating_weight * (rating - 3.0) / 2.0)\n",
        "            product_to_score[asin] = product_to_score.get(asin, 0.0) + review_score\n",
        "            product_to_hits[asin] = product_to_hits.get(asin, 0) + 1\n",
        "    items = []\n",
        "    for asin, tot in product_to_score.items():\n",
        "        hits = product_to_hits.get(asin, 1)\n",
        "        items.append((asin, tot / float(hits)))\n",
        "    if not items:\n",
        "        rated = df[[\"asin\", \"overall\"]].groupby(\"asin\").mean()[\"overall\"].sort_values(ascending=False)\n",
        "        items = [(a, float(score)) for a, score in rated.head(k_recommendations).items()]\n",
        "        return items\n",
        "    items.sort(key=lambda x: x[1], reverse=True)\n",
        "    return items[:k_recommendations]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your CSV from Google Drive or local upload\n",
        "# Option A: Upload via Colab UI (left sidebar > Files)\n",
        "# csv_path = \"/content/amazon_reviews.csv\"\n",
        "\n",
        "# Option B: Mount Drive and set path\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#csv_path = \"/content/drive/MyDrive/amazon_reviews.csv\"\n",
        "\n",
        "# For demo, create a tiny sample if you haven't uploaded yet\n",
        "import os\n",
        "csv_path = os.environ.get(\"CSV_PATH\", \"/content/amazon_reviews.csv\")\n",
        "if not os.path.exists(csv_path):\n",
        "    import io\n",
        "    sample = io.StringIO(\"\"\"userName,itemName,description,image,brand,feature,category,price,rating,reviewTime,summary,reviewText,vote\n",
        "u1,Sample Table,desc,,BrandA,,Furniture,49.9,5,2024-01-01,Great,Sturdy table and looks nice,10\n",
        "u2,Study Chair,desc,,BrandB,,Furniture,39.9,4,2024-01-02,Good,Comfortable for long hours,5\n",
        "u3,Table Lamp,desc,,BrandC,,Lighting,19.9,4,2024-01-03,Nice,Soft light and elegant,3\n",
        "\"\"\")\n",
        "    with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(sample.getvalue())\n",
        "\n",
        "# Column map to adapt to your headers\n",
        "column_map = {\n",
        "    \"id\": \"asin\",      # if you don't have an ID, we'll fallback to itemName\n",
        "    \"title\": \"itemName\",\n",
        "    \"rating\": \"rating\",\n",
        "    \"review\": \"reviewText\",\n",
        "    \"user\": \"userName\",\n",
        "    \"summary\": \"summary\",\n",
        "    \"category\": \"category\",\n",
        "    \"brand\": \"brand\",\n",
        "}\n",
        "\n",
        "raw = pd.read_csv(csv_path)\n",
        "\n",
        "def remap_columns(df: pd.DataFrame, column_map: dict) -> pd.DataFrame:\n",
        "    mapped = pd.DataFrame()\n",
        "    id_col = column_map.get(\"id\")\n",
        "    if id_col and id_col in df.columns:\n",
        "        mapped[\"asin\"] = df[id_col].astype(str)\n",
        "    else:\n",
        "        mapped[\"asin\"] = df[column_map.get(\"title\", \"itemName\")].astype(str)\n",
        "    mapped[\"title\"] = df.get(column_map.get(\"title\", \"itemName\"), \"\").astype(str)\n",
        "    mapped[\"reviewText\"] = df.get(column_map.get(\"review\", \"reviewText\"), \"\").astype(str)\n",
        "    mapped[\"overall\"] = pd.to_numeric(df.get(column_map.get(\"rating\", \"rating\"), 0), errors=\"coerce\").fillna(0)\n",
        "    return mapped\n",
        "\n",
        "print(raw.head(3))\n",
        "df = remap_columns(raw, column_map)\n",
        "# Clean\n",
        "df = df.dropna(subset=[\"reviewText\"]).copy()\n",
        "df[\"overall\"] = pd.to_numeric(df[\"overall\"], errors=\"coerce\").fillna(0)\n",
        "df[\"reviewText\"] = df[\"reviewText\"].astype(str).str.slice(0, 512)\n",
        "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
        "df = df.reset_index(drop=True)\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment + Embeddings + Simple NumPy index\n",
        "sent_model = VaderSentiment()\n",
        "df[\"sentiment\"] = sent_model.score_texts(df[\"reviewText\"].tolist())\n",
        "\n",
        "emb_model = EmbeddingModel(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embs = emb_model.encode(df[\"reviewText\"].tolist(), batch_size=256)\n",
        "\n",
        "# Query function\n",
        "def search_reviews(query: str, top_k: int = 50):\n",
        "    q = emb_model.encode([query], batch_size=1)\n",
        "    # cosine via dot on normalized vectors\n",
        "    sim = (q @ embs.T)[0]\n",
        "    # take top_k\n",
        "    idx = np.argpartition(-sim, kth=min(top_k, len(sim)-1))[:top_k]\n",
        "    idx = idx[np.argsort(-sim[idx])]\n",
        "    scores = sim[idx]\n",
        "    return scores, idx\n",
        "\n",
        "# Recommend function\n",
        "def recommend(query: str, top_k: int = 50, n_recs: int = 10, sent_min: float = 0.1, rating_min: float = 4.0):\n",
        "    scores, idx = search_reviews(query, top_k=top_k)\n",
        "    recs = aggregate_product_scores(\n",
        "        df=df,\n",
        "        retrieved_indices=np.array([idx]),\n",
        "        retrieved_scores=np.array([scores]),\n",
        "        review_sentiments=df[\"sentiment\"].to_numpy(),\n",
        "        k_recommendations=n_recs,\n",
        "        sentiment_weight=0.6,\n",
        "        rating_weight=0.4,\n",
        "        sentiment_min=sent_min,\n",
        "        rating_min=rating_min,\n",
        "    )\n",
        "    return scores, idx, recs\n",
        "\n",
        "print(\"Ready. Try recommend('table')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio UI for search + recommendations\n",
        "import gradio as gr\n",
        "\n",
        "def ui_recommend(query, top_k, n_recs, sent_min, rating_min):\n",
        "    if not query:\n",
        "        return \"\", \"Enter a query\"\n",
        "    scores, idx, recs = recommend(query, int(top_k), int(n_recs), float(sent_min), float(rating_min))\n",
        "    # Top reviews block\n",
        "    lines = []\n",
        "    shown = 0\n",
        "    for j, i in enumerate(idx):\n",
        "        if shown >= min(20, len(idx)):\n",
        "            break\n",
        "        if df.loc[i, \"overall\"] >= float(rating_min) and df.loc[i, \"sentiment\"] >= float(sent_min) and float(scores[j]) > 0:\n",
        "            lines.append(f\"- [{df.loc[i, 'title']}] {df.loc[i, 'reviewText']}\")\n",
        "            shown += 1\n",
        "    if not lines:\n",
        "        lines.append(\"No positive reviews matched filters. Try lowering thresholds.\")\n",
        "    # Recs\n",
        "    rec_lines = [f\"{i+1}. {asin} | score={score:.4f}\" for i, (asin, score) in enumerate(recs)]\n",
        "    return \"\\n\".join(lines), \"\\n\".join(rec_lines)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Amazon Reviews IR + Recommender (Colab)\")\n",
        "    with gr.Row():\n",
        "        query = gr.Textbox(label=\"Query\", value=\"table for study\")\n",
        "    with gr.Row():\n",
        "        top_k = gr.Slider(10, 200, value=50, step=10, label=\"Top K Reviews\")\n",
        "        n_recs = gr.Slider(3, 20, value=10, step=1, label=\"Number of recommendations\")\n",
        "    with gr.Row():\n",
        "        sent_min = gr.Slider(-1.0, 1.0, value=0.1, step=0.05, label=\"Minimum sentiment\")\n",
        "        rating_min = gr.Slider(1, 5, value=4, step=1, label=\"Minimum rating\")\n",
        "    run_btn = gr.Button(\"Search & Recommend\")\n",
        "    reviews_out = gr.Textbox(label=\"Top Reviews\", lines=12)\n",
        "    recs_out = gr.Textbox(label=\"Recommended Products\", lines=12)\n",
        "    run_btn.click(ui_recommend, inputs=[query, top_k, n_recs, sent_min, rating_min], outputs=[reviews_out, recs_out])\n",
        "\n",
        "demo.launch(share=False)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
